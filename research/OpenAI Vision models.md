I'll search for the latest information about OpenAI models used for vision and image understanding.

















Based on my search, here are the **OpenAI models used for vision and image understanding** as of 2025:

## Current Vision-Enabled Models

**Primary Vision Models:**
- **GPT-4o series** - Latest flagship model with strong vision capabilities
- **GPT-4o mini** - Cost-efficient version with vision support
- **GPT-4.1 series** - Updated vision models including GPT-4.1-mini and GPT-4.1-nano
- **o-series reasoning models** - New reasoning models with vision capabilities
- **GPT-5 series** - Latest generation with enhanced vision

## Key Vision Capabilities

**Supported Input Formats:**
- Image URLs (HTTP/HTTPS)
- Base64 encoded images
- Multiple images in single request
- Detail levels: "low", "high", "auto"

**Cost Structure (GPT-4o and o-series):**
- **Low detail**: Fixed 85 tokens per image
- **High detail**: 170 tokens per 512px tile + 85 tokens base
- Example: 1024x1024 image = 765 tokens (high detail)

## Performance Comparison

**GPT-4o vs GPT-4o mini:**
- **GPT-4o**: Higher accuracy, better comprehension, higher cost
- **GPT-4o mini**: Cost-efficient, good for most tasks, faster response
- Both support text extraction, object recognition, and image captioning

## Limitations

- Not suitable for medical image interpretation
- Struggles with non-Latin text in images
- Limited spatial reasoning for precise localization
- May have issues with rotated text or complex graphs
- Panoramic and fisheye images not well supported

The **GPT-4o series** is currently recommended for best vision performance, while **GPT-4o mini** offers a cost-effective alternative for less demanding vision tasks.